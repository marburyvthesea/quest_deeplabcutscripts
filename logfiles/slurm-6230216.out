Module is experimental. 
config file is: /projects/p30771/dlc_analysis/operant_box_habituation-JJM-2020-08-03/config.yaml
/home/jma819/.conda/envs/tensorflow-gpu-env/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/jma819/.conda/envs/tensorflow-gpu-env/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/jma819/.conda/envs/tensorflow-gpu-env/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/jma819/.conda/envs/tensorflow-gpu-env/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/jma819/.conda/envs/tensorflow-gpu-env/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/jma819/.conda/envs/tensorflow-gpu-env/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
Config:
{'all_joints': [[0], [1], [2], [3], [4], [5], [6], [7], [8]],
 'all_joints_names': ['ear_left',
                      'ear_right',
                      'tail_base',
                      'snout',
                      'paw_left',
                      'paw_right',
                      'panel_1',
                      'panel_2',
                      'spout'],
 'batch_size': 1,
 'bottomheight': 400,
 'crop': True,
 'crop_pad': 0,
 'cropratio': 0.4,
 'dataset': 'training-datasets/iteration-0/UnaugmentedDataSet_operant_box_habituationAug3/operant_box_habituation_JJM95shuffle1.mat',
 'dataset_type': 'default',
 'deterministic': False,
 'display_iters': 1000,
 'fg_fraction': 0.25,
 'global_scale': 0.8,
 'init_weights': '/home/jma819/DeepLabCut/deeplabcut/pose_estimation_tensorflow/models/pretrained/resnet_v1_50.ckpt',
 'intermediate_supervision': False,
 'intermediate_supervision_layer': 12,
 'leftwidth': 400,
 'location_refinement': True,
 'locref_huber_loss': True,
 'locref_loss_weight': 0.05,
 'locref_stdev': 7.2801,
 'log_dir': 'log',
 'max_input_size': 1500,
 'mean_pixel': [123.68, 116.779, 103.939],
 'metadataset': 'training-datasets/iteration-0/UnaugmentedDataSet_operant_box_habituationAug3/Documentation_data-operant_box_habituation_95shuffle1.pickle',
 'min_input_size': 64,
 'minsize': 100,
 'mirror': False,
 'multi_step': [[0.005, 10000],
                [0.02, 430000],
                [0.002, 730000],
                [0.001, 1030000]],
 'net_type': 'resnet_50',
 'num_joints': 9,
 'optimizer': 'sgd',
 'pos_dist_thresh': 17,
 'project_path': '/projects/p30771/dlc_analysis/operant_box_habituation-JJM-2020-08-03',
 'regularize': False,
 'rightwidth': 400,
 'save_iters': 50000,
 'scale_jitter_lo': 0.5,
 'scale_jitter_up': 1.25,
 'scoremap_dir': 'test',
 'shuffle': True,
 'snapshot_prefix': '/projects/p30771/dlc_analysis/operant_box_habituation-JJM-2020-08-03/dlc-models/iteration-0/operant_box_habituationAug3-trainset95shuffle1/train/snapshot',
 'stride': 8.0,
 'topheight': 400,
 'weigh_negatives': False,
 'weigh_only_present_joints': False,
 'weigh_part_predictions': False,
 'weight_decay': 0.0001}
iteration: 1000 loss: 0.0196 lr: 0.005
iteration: 2000 loss: 0.0081 lr: 0.005
iteration: 3000 loss: 0.0061 lr: 0.005
iteration: 4000 loss: 0.0053 lr: 0.005
iteration: 5000 loss: 0.0044 lr: 0.005
iteration: 6000 loss: 0.0041 lr: 0.005
iteration: 7000 loss: 0.0038 lr: 0.005
iteration: 8000 loss: 0.0036 lr: 0.005
iteration: 9000 loss: 0.0034 lr: 0.005
iteration: 10000 loss: 0.0031 lr: 0.005
iteration: 11000 loss: 0.0050 lr: 0.02
iteration: 12000 loss: 0.0038 lr: 0.02
iteration: 13000 loss: 0.0032 lr: 0.02
iteration: 14000 loss: 0.0028 lr: 0.02
iteration: 15000 loss: 0.0027 lr: 0.02
iteration: 16000 loss: 0.0025 lr: 0.02
iteration: 17000 loss: 0.0023 lr: 0.02
iteration: 18000 loss: 0.0021 lr: 0.02
iteration: 19000 loss: 0.0020 lr: 0.02
iteration: 20000 loss: 0.0020 lr: 0.02
iteration: 21000 loss: 0.0019 lr: 0.02
iteration: 22000 loss: 0.0019 lr: 0.02
iteration: 23000 loss: 0.0019 lr: 0.02
iteration: 24000 loss: 0.0018 lr: 0.02
iteration: 25000 loss: 0.0017 lr: 0.02
iteration: 26000 loss: 0.0017 lr: 0.02
iteration: 27000 loss: 0.0016 lr: 0.02
iteration: 28000 loss: 0.0016 lr: 0.02
iteration: 29000 loss: 0.0017 lr: 0.02
iteration: 30000 loss: 0.0016 lr: 0.02
iteration: 31000 loss: 0.0016 lr: 0.02
iteration: 32000 loss: 0.0015 lr: 0.02
iteration: 33000 loss: 0.0015 lr: 0.02
iteration: 34000 loss: 0.0015 lr: 0.02
iteration: 35000 loss: 0.0015 lr: 0.02
iteration: 36000 loss: 0.0015 lr: 0.02
iteration: 37000 loss: 0.0014 lr: 0.02
iteration: 38000 loss: 0.0015 lr: 0.02
iteration: 39000 loss: 0.0015 lr: 0.02
iteration: 40000 loss: 0.0014 lr: 0.02
iteration: 41000 loss: 0.0015 lr: 0.02
iteration: 42000 loss: 0.0014 lr: 0.02
iteration: 43000 loss: 0.0014 lr: 0.02
iteration: 44000 loss: 0.0014 lr: 0.02
iteration: 45000 loss: 0.0014 lr: 0.02
iteration: 46000 loss: 0.0014 lr: 0.02
iteration: 47000 loss: 0.0014 lr: 0.02
iteration: 48000 loss: 0.0013 lr: 0.02
iteration: 49000 loss: 0.0013 lr: 0.02
iteration: 50000 loss: 0.0013 lr: 0.02
iteration: 51000 loss: 0.0013 lr: 0.02
iteration: 52000 loss: 0.0013 lr: 0.02
iteration: 53000 loss: 0.0013 lr: 0.02
iteration: 54000 loss: 0.0012 lr: 0.02
iteration: 55000 loss: 0.0013 lr: 0.02
iteration: 56000 loss: 0.0013 lr: 0.02
iteration: 57000 loss: 0.0013 lr: 0.02
iteration: 58000 loss: 0.0013 lr: 0.02
iteration: 59000 loss: 0.0013 lr: 0.02
iteration: 60000 loss: 0.0012 lr: 0.02
iteration: 61000 loss: 0.0013 lr: 0.02
iteration: 62000 loss: 0.0012 lr: 0.02
iteration: 63000 loss: 0.0012 lr: 0.02
iteration: 64000 loss: 0.0012 lr: 0.02
iteration: 65000 loss: 0.0012 lr: 0.02
iteration: 66000 loss: 0.0012 lr: 0.02
iteration: 67000 loss: 0.0013 lr: 0.02
iteration: 68000 loss: 0.0012 lr: 0.02
iteration: 69000 loss: 0.0012 lr: 0.02
iteration: 70000 loss: 0.0013 lr: 0.02
iteration: 71000 loss: 0.0012 lr: 0.02
iteration: 72000 loss: 0.0012 lr: 0.02
iteration: 73000 loss: 0.0012 lr: 0.02
iteration: 74000 loss: 0.0012 lr: 0.02
iteration: 75000 loss: 0.0012 lr: 0.02
iteration: 76000 loss: 0.0011 lr: 0.02
iteration: 77000 loss: 0.0012 lr: 0.02
iteration: 78000 loss: 0.0012 lr: 0.02
iteration: 79000 loss: 0.0012 lr: 0.02
iteration: 80000 loss: 0.0012 lr: 0.02
iteration: 81000 loss: 0.0011 lr: 0.02
iteration: 82000 loss: 0.0012 lr: 0.02
iteration: 83000 loss: 0.0011 lr: 0.02
iteration: 84000 loss: 0.0011 lr: 0.02
iteration: 85000 loss: 0.0011 lr: 0.02
iteration: 86000 loss: 0.0011 lr: 0.02
iteration: 87000 loss: 0.0011 lr: 0.02
iteration: 88000 loss: 0.0011 lr: 0.02
iteration: 89000 loss: 0.0012 lr: 0.02
iteration: 90000 loss: 0.0011 lr: 0.02
iteration: 91000 loss: 0.0011 lr: 0.02
iteration: 92000 loss: 0.0011 lr: 0.02
iteration: 93000 loss: 0.0011 lr: 0.02
iteration: 94000 loss: 0.0011 lr: 0.02
iteration: 95000 loss: 0.0011 lr: 0.02
iteration: 96000 loss: 0.0011 lr: 0.02
iteration: 97000 loss: 0.0011 lr: 0.02
iteration: 98000 loss: 0.0011 lr: 0.02
iteration: 99000 loss: 0.0011 lr: 0.02
iteration: 100000 loss: 0.0011 lr: 0.02
iteration: 101000 loss: 0.0011 lr: 0.02
iteration: 102000 loss: 0.0011 lr: 0.02
iteration: 103000 loss: 0.0011 lr: 0.02
iteration: 104000 loss: 0.0011 lr: 0.02
iteration: 105000 loss: 0.0011 lr: 0.02
iteration: 106000 loss: 0.0011 lr: 0.02
iteration: 107000 loss: 0.0011 lr: 0.02
iteration: 108000 loss: 0.0011 lr: 0.02
iteration: 109000 loss: 0.0011 lr: 0.02
iteration: 110000 loss: 0.0011 lr: 0.02
iteration: 111000 loss: 0.0011 lr: 0.02
iteration: 112000 loss: 0.0011 lr: 0.02
iteration: 113000 loss: 0.0011 lr: 0.02
iteration: 114000 loss: 0.0011 lr: 0.02
iteration: 115000 loss: 0.0011 lr: 0.02
iteration: 116000 loss: 0.0011 lr: 0.02
iteration: 117000 loss: 0.0011 lr: 0.02
iteration: 118000 loss: 0.0011 lr: 0.02
iteration: 119000 loss: 0.0010 lr: 0.02
iteration: 120000 loss: 0.0010 lr: 0.02
iteration: 121000 loss: 0.0010 lr: 0.02
iteration: 122000 loss: 0.0010 lr: 0.02
iteration: 123000 loss: 0.0011 lr: 0.02
iteration: 124000 loss: 0.0011 lr: 0.02
iteration: 125000 loss: 0.0011 lr: 0.02
iteration: 126000 loss: 0.0011 lr: 0.02
iteration: 127000 loss: 0.0010 lr: 0.02
iteration: 128000 loss: 0.0010 lr: 0.02
iteration: 129000 loss: 0.0010 lr: 0.02
iteration: 130000 loss: 0.0010 lr: 0.02
iteration: 131000 loss: 0.0010 lr: 0.02
iteration: 132000 loss: 0.0010 lr: 0.02
iteration: 133000 loss: 0.0011 lr: 0.02
iteration: 134000 loss: 0.0010 lr: 0.02
iteration: 135000 loss: 0.0010 lr: 0.02
iteration: 136000 loss: 0.0010 lr: 0.02
iteration: 137000 loss: 0.0011 lr: 0.02
iteration: 138000 loss: 0.0010 lr: 0.02
iteration: 139000 loss: 0.0011 lr: 0.02
iteration: 140000 loss: 0.0010 lr: 0.02
iteration: 141000 loss: 0.0010 lr: 0.02
iteration: 142000 loss: 0.0010 lr: 0.02
iteration: 143000 loss: 0.0010 lr: 0.02
iteration: 144000 loss: 0.0010 lr: 0.02
iteration: 145000 loss: 0.0010 lr: 0.02
iteration: 146000 loss: 0.0010 lr: 0.02
iteration: 147000 loss: 0.0010 lr: 0.02
iteration: 148000 loss: 0.0010 lr: 0.02
iteration: 149000 loss: 0.0010 lr: 0.02
iteration: 150000 loss: 0.0010 lr: 0.02
iteration: 151000 loss: 0.0010 lr: 0.02
iteration: 152000 loss: 0.0010 lr: 0.02
iteration: 153000 loss: 0.0010 lr: 0.02
iteration: 154000 loss: 0.0010 lr: 0.02
iteration: 155000 loss: 0.0010 lr: 0.02
iteration: 156000 loss: 0.0010 lr: 0.02
iteration: 157000 loss: 0.0010 lr: 0.02
iteration: 158000 loss: 0.0010 lr: 0.02
iteration: 159000 loss: 0.0010 lr: 0.02
iteration: 160000 loss: 0.0010 lr: 0.02
iteration: 161000 loss: 0.0010 lr: 0.02
iteration: 162000 loss: 0.0009 lr: 0.02
iteration: 163000 loss: 0.0010 lr: 0.02
iteration: 164000 loss: 0.0009 lr: 0.02
iteration: 165000 loss: 0.0010 lr: 0.02
iteration: 166000 loss: 0.0009 lr: 0.02
iteration: 167000 loss: 0.0010 lr: 0.02
iteration: 168000 loss: 0.0010 lr: 0.02
iteration: 169000 loss: 0.0010 lr: 0.02
iteration: 170000 loss: 0.0010 lr: 0.02
iteration: 171000 loss: 0.0009 lr: 0.02
iteration: 172000 loss: 0.0010 lr: 0.02
iteration: 173000 loss: 0.0010 lr: 0.02
iteration: 174000 loss: 0.0010 lr: 0.02
iteration: 175000 loss: 0.0009 lr: 0.02
iteration: 176000 loss: 0.0010 lr: 0.02
iteration: 177000 loss: 0.0009 lr: 0.02
iteration: 178000 loss: 0.0009 lr: 0.02
iteration: 179000 loss: 0.0010 lr: 0.02
iteration: 180000 loss: 0.0009 lr: 0.02
iteration: 181000 loss: 0.0009 lr: 0.02
iteration: 182000 loss: 0.0009 lr: 0.02
iteration: 183000 loss: 0.0009 lr: 0.02
iteration: 184000 loss: 0.0009 lr: 0.02
iteration: 185000 loss: 0.0009 lr: 0.02
iteration: 186000 loss: 0.0009 lr: 0.02
iteration: 187000 loss: 0.0009 lr: 0.02
iteration: 188000 loss: 0.0009 lr: 0.02
iteration: 189000 loss: 0.0009 lr: 0.02
iteration: 190000 loss: 0.0009 lr: 0.02
iteration: 191000 loss: 0.0010 lr: 0.02
iteration: 192000 loss: 0.0009 lr: 0.02
iteration: 193000 loss: 0.0009 lr: 0.02
iteration: 194000 loss: 0.0009 lr: 0.02
iteration: 195000 loss: 0.0009 lr: 0.02
iteration: 196000 loss: 0.0009 lr: 0.02
iteration: 197000 loss: 0.0009 lr: 0.02
iteration: 198000 loss: 0.0009 lr: 0.02
iteration: 199000 loss: 0.0009 lr: 0.02
iteration: 200000 loss: 0.0009 lr: 0.02
iteration: 201000 loss: 0.0009 lr: 0.02
iteration: 202000 loss: 0.0009 lr: 0.02
iteration: 203000 loss: 0.0009 lr: 0.02
iteration: 204000 loss: 0.0009 lr: 0.02
iteration: 205000 loss: 0.0009 lr: 0.02
iteration: 206000 loss: 0.0009 lr: 0.02
iteration: 207000 loss: 0.0009 lr: 0.02
iteration: 208000 loss: 0.0009 lr: 0.02
iteration: 209000 loss: 0.0009 lr: 0.02
iteration: 210000 loss: 0.0009 lr: 0.02
iteration: 211000 loss: 0.0009 lr: 0.02
iteration: 212000 loss: 0.0009 lr: 0.02
iteration: 213000 loss: 0.0009 lr: 0.02
iteration: 214000 loss: 0.0009 lr: 0.02
iteration: 215000 loss: 0.0009 lr: 0.02
iteration: 216000 loss: 0.0009 lr: 0.02
iteration: 217000 loss: 0.0009 lr: 0.02
iteration: 218000 loss: 0.0009 lr: 0.02
iteration: 219000 loss: 0.0009 lr: 0.02
iteration: 220000 loss: 0.0009 lr: 0.02
iteration: 221000 loss: 0.0009 lr: 0.02
iteration: 222000 loss: 0.0009 lr: 0.02
iteration: 223000 loss: 0.0009 lr: 0.02
iteration: 224000 loss: 0.0008 lr: 0.02
iteration: 225000 loss: 0.0009 lr: 0.02
iteration: 226000 loss: 0.0009 lr: 0.02
iteration: 227000 loss: 0.0009 lr: 0.02
iteration: 228000 loss: 0.0009 lr: 0.02
iteration: 229000 loss: 0.0008 lr: 0.02
iteration: 230000 loss: 0.0009 lr: 0.02
iteration: 231000 loss: 0.0009 lr: 0.02
iteration: 232000 loss: 0.0009 lr: 0.02
iteration: 233000 loss: 0.0009 lr: 0.02
iteration: 234000 loss: 0.0008 lr: 0.02
iteration: 235000 loss: 0.0009 lr: 0.02
iteration: 236000 loss: 0.0009 lr: 0.02
iteration: 237000 loss: 0.0008 lr: 0.02
iteration: 238000 loss: 0.0008 lr: 0.02
iteration: 239000 loss: 0.0009 lr: 0.02
iteration: 240000 loss: 0.0009 lr: 0.02
iteration: 241000 loss: 0.0008 lr: 0.02
iteration: 242000 loss: 0.0009 lr: 0.02
iteration: 243000 loss: 0.0009 lr: 0.02
iteration: 244000 loss: 0.0009 lr: 0.02
iteration: 245000 loss: 0.0009 lr: 0.02
iteration: 246000 loss: 0.0009 lr: 0.02
iteration: 247000 loss: 0.0009 lr: 0.02
iteration: 248000 loss: 0.0008 lr: 0.02
iteration: 249000 loss: 0.0008 lr: 0.02
iteration: 250000 loss: 0.0008 lr: 0.02
iteration: 251000 loss: 0.0008 lr: 0.02
iteration: 252000 loss: 0.0008 lr: 0.02
iteration: 253000 loss: 0.0008 lr: 0.02
iteration: 254000 loss: 0.0008 lr: 0.02
iteration: 255000 loss: 0.0008 lr: 0.02
iteration: 256000 loss: 0.0008 lr: 0.02
iteration: 257000 loss: 0.0008 lr: 0.02
iteration: 258000 loss: 0.0008 lr: 0.02
slurmstepd: error: *** JOB 6230216 ON qgpu6015 CANCELLED AT 2020-08-05T10:29:54 DUE TO TIME LIMIT ***
